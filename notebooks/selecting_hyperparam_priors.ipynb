{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem?\\\n",
    "1-task ICM does not perform as well as SOGP in BOSS MT\\\n",
    "Solution?\\\n",
    "Fix hyperparameters so that $\\bf{B} = [var]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance functions\n",
    "\n",
    "## RBF\n",
    "\n",
    "$k_{RBF}(x,x') = \\sigma^2e^{-(x-x')^2/2L^2}$\n",
    "\n",
    "where\\\n",
    "$\\sigma^2 = $ variance, scaling factor\\\n",
    "$L = $ lenghtscale, 'smoothness': large $L$ = smooth curve\n",
    "\n",
    "## STDP\n",
    "\n",
    "$k_{STDP}(x,x') = \\sigma^2e^{-2\\text{sin}^2(\\pi|x-x'|)/pL^2}$\n",
    "\n",
    "where\\\n",
    "$p = $ period\n",
    "\n",
    "covariance functions may also be more complex, or they can be added or multiplied together for example to add support for multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOGP kernel\n",
    "\n",
    "$\\bf{K} = \\begin{pmatrix}\n",
    "    k(\\textbf{x}_1,\\textbf{x}_1) & \\dots & k(\\textbf{x}_1,\\textbf{x}_N) \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    k(\\textbf{x}_N,\\textbf{x}_1) & \\dots & k(\\textbf{x}_N,\\textbf{x}_N) \\\\\n",
    "    \\end{pmatrix}$,\n",
    "\n",
    "where $N = $ number of inputs and\n",
    "$k(\\bf{x},\\bf{x}') = $ valid covariance function for vector input $\\bf{x} \\in \\mathbf{R}^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coregionalization kernels\n",
    "\n",
    "Kernel and coregionalization matrix produce MT kernel.\\\n",
    "Now coregionalization matrix consumes the variance $\\sigma^2$, so it is fixed to one in the covariance functions.\n",
    "\n",
    "## LCM kernel\n",
    "\n",
    "$\\bf{K}_{LMC} = \\sum_{q=1}^{Q}\\bf{B}_q\\otimes\\bf{K}_q$ (see Kronecker product below)\n",
    "\n",
    "## ICM kernel (Q = 1)\n",
    "\n",
    "$\\bf{K}_{IMC} = \\bf{B}_1\\otimes\\bf{K}_1$\n",
    "\n",
    "### Coregionalization matrix $\\bf{B}_q$:\n",
    "$\\bf{B}_q = \\bf{W}_q\\bf{W}_q^\\top$\\\n",
    "or (how it is in GPy, I have not yet found original source to this, althought it makes sense)\\\n",
    "$\\bf{B}_q = \\bf{W}_q\\bf{W}_q^\\top + diag(\\bf{\\kappa})$\n",
    "\n",
    "where:\\\n",
    "$\\bf{W}_q \\in \\mathbf{R}^{d\\times r}$ encodes correlation between tasks,\\\n",
    "$\\bf{\\kappa} \\in \\mathbf{R}^r$ allows independence (or variance) of tasks, and\\\n",
    "$r \\in \\{1,...,num\\_tasks\\}$ is a grouping factor that may be used to group tasks that share covariance functions\n",
    "\n",
    "### Kronecker product $\\otimes$\n",
    "\n",
    "$\\begin{pmatrix} a & b \\\\ c & d\\end{pmatrix} \\otimes \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} = \\begin{pmatrix}  a1 & a2 & b1 & b2 \\\\ a3 & a4 & b3 & b4 \\\\ c1 & c2 & d1 & d2 \\\\ c3 & c4 & d3 & d4 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 5],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([1,2])\n",
    "k = np.array([4,1])\n",
    "W.dot(A.transpose())+np.diag(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPr\n",
    "\n",
    "Prior predictive for new observations:\n",
    "\n",
    "\\begin{equation}p(y_*,\\textbf{y}|\\textbf{x}_*,\\textbf{X})\n",
    "    \\sim\n",
    "    N\\begin{pmatrix}\n",
    "    \\begin{matrix}\n",
    "    m(\\textbf{x}_1) \\\\ \\vdots \\\\ m(\\textbf{x}_j) \\\\ m(\\textbf{x}_*)\n",
    "    \\end{matrix}\n",
    "    ,\n",
    "    \\begin{matrix}\n",
    "        \\begin{pmatrix}\n",
    "        k(\\textbf{x}_1,\\textbf{x}_1) & \\dots & k(\\textbf{x}_1,\\textbf{x}_j) \\\\\n",
    "        \\vdots & \\ddots & \\vdots \\\\\n",
    "        k(\\textbf{x}_j,\\textbf{x}_1) & \\dots & k(\\textbf{x}_j,\\textbf{x}_j) \\\\\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "        k(\\textbf{x}_1,\\textbf{x}_*) \\\\ \\vdots \\\\ k(\\textbf{x}_j,\\textbf{x}_*)\n",
    "        \\end{pmatrix}\n",
    "        \\\\\n",
    "        \\begin{pmatrix}\n",
    "        k(\\textbf{x}_*,\\textbf{x}_1) & \\dots & k(\\textbf{x}_*,\\textbf{x}_j) \\\\\n",
    "        \\end{pmatrix}\n",
    "        \\begin{pmatrix}\n",
    "        k(\\textbf{x}_*,\\textbf{x}_*)\n",
    "        \\end{pmatrix}\n",
    "    \\end{matrix}\n",
    "    \\end{pmatrix}\n",
    "    =\n",
    "    N\\begin{pmatrix}\n",
    "    \\begin{vmatrix}\n",
    "    \\bf{\\mu} \\\\ \\mu_*\n",
    "    \\end{vmatrix}\n",
    "    ,\n",
    "    \\begin{vmatrix}\n",
    "        \\mathbf{K}\n",
    "        & \n",
    "        \\mathbf{K_*^\\top}\n",
    "        \\\\\n",
    "        \\mathbf{K_*}\n",
    "        &\n",
    "        \\mathbf{K_{**}}\n",
    "    \\end{vmatrix}\n",
    "    \\end{pmatrix}\n",
    "    .\n",
    "\\end{equation}\n",
    "\n",
    "Posterior predictive:\\\n",
    "\\begin{equation}\n",
    "     p(y_*|\\textbf{x}_*,\\textbf{X},\\textbf{y}) \\sim N(\\hat\\mu,\\hat\\nu).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution parameters can be solved from:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{muhatsolution}\n",
    "    \\hat\\mu = \\mathbf{K}_*^\\top\\mathbf{K}^{-1}(\\textbf{y}-\\bf{\\mu})+\\mu_*\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\label{nuhatsolution}\n",
    "    \\hat\\nu = \\mathbf{K}_*^\\top\\mathbf{K}^{-1}\\mathbf{K}_* + \\mathbf{K}_{**}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOSS\n",
    "Covariance function:\n",
    "\n",
    "Hyperparameter priors:\n",
    "$d = y_{max}-y_{min}$\\\n",
    "$var = gammadist(2, 2/(d/2)^2)$\\\n",
    "$lengthscales = [gammadist(3.3678, 9.0204)_1, ..., gammadist(3.3678, 9.0204)_{dim}]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### STS thetainit\n",
    "# default initial hyperparameters\n",
    "        if self.thetainit is None:\n",
    "            self.thetainit = [0.5 * (self.yrange[1] - self.yrange[0])] # sig\n",
    "            for i in range(self.dim): # lengthscales\n",
    "                if self.kerntype[i] == 'stdp': # pbc\n",
    "                    self.thetainit.append(np.pi/10)\n",
    "                else: # nonpbc\n",
    "                    self.thetainit.append(self.periods[i]/20)\n",
    "\n",
    "\n",
    "### kernelfactory\n",
    "for i in range(STS.dim):\n",
    "            if forced_hypers is None:\n",
    "                if i == 0:\n",
    "                    Ksi = STS.thetainit[0]\n",
    "                else:\n",
    "                    Ksi = 1.0\n",
    "                Klsi = STS.thetainit[i + 1]\n",
    "            else:\n",
    "                Ksi = forced_hypers[0]\n",
    "                Klsi = forced_hypers[1 + i]\n",
    "            Kp = STS.periods[i]\n",
    "\n",
    "            if STS.kerntype[i] == \"stdp\":\n",
    "                kerns[i] = GPy.kern.StdPeriodic( ##########################\n",
    "                    input_dim=1,\n",
    "                    variance=Ksi,\n",
    "                    period=Kp,\n",
    "                    lengthscale=Klsi,\n",
    "                    ARD1=True,\n",
    "                    ARD2=True,\n",
    "                    active_dims=[i],\n",
    "                    name=\"kern\",\n",
    "                )\n",
    "                \n",
    "### GPy stdperiodic\n",
    "def __init__(self, input_dim, variance=1., period=None, lengthscale=None, ARD1=False, ARD2=False, active_dims=None, name='std_periodic',useGPU=False):\n",
    "        super(StdPeriodic, self).__init__(input_dim, active_dims, name, useGPU=useGPU)\n",
    "        self.ARD1 = ARD1 # correspond to periods\n",
    "        self.ARD2 = ARD2 # correspond to lengthscales\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        if self.ARD1 == False:\n",
    "            if period is not None: #### shared\n",
    "                period = np.asarray(period)\n",
    "                assert period.size == 1, \"Only one period needed for non-ARD kernel\"\n",
    "            else:\n",
    "                period = np.ones(1)\n",
    "        else:\n",
    "            if period is not None:\n",
    "                period = np.asarray(period)\n",
    "                assert period.size == input_dim, \"bad number of periods\"\n",
    "            else:\n",
    "                period = np.ones(input_dim)\n",
    "\n",
    "        if self.ARD2 == False: ##### shared\n",
    "            if lengthscale is not None:\n",
    "                lengthscale = np.asarray(lengthscale)\n",
    "                assert lengthscale.size == 1, \"Only one lengthscale needed for non-ARD kernel\"\n",
    "            else:\n",
    "                lengthscale = np.ones(1)\n",
    "        else:\n",
    "            if lengthscale is not None:\n",
    "                lengthscale = np.asarray(lengthscale)\n",
    "                assert lengthscale.size == input_dim, \"bad number of lengthscales\"\n",
    "            else:\n",
    "                lengthscale = np.ones(input_dim)\n",
    "\n",
    "        self.variance = Param('variance', variance, Logexp()) ############### variance\n",
    "        assert self.variance.size==1, \"Variance size must be one\"\n",
    "        self.period =  Param('period', period, Logexp())\n",
    "        self.lengthscale =  Param('lengthscale', lengthscale, Logexp())\n",
    "\n",
    "        self.link_parameters(self.variance,  self.period, self.lengthscale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOSS-MT (currently)\n",
    "GPy defaults\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- default hyperparameter priors\n",
    "        if self.thetapriorpar is None and self.thetaprior is not None:\n",
    "            if self.thetaprior == \"gamma\":\n",
    "                self.thetapriorpar = [[1,1]] # not used, but must be passed to kernelfactory initialized\n",
    "            else:\n",
    "                raise TypeError(\"Unknown options set for thetaprior: '\"\n",
    "                                + self.thetaprior + \"'.\")\n",
    "\n",
    "            for i in range(self.dim):\n",
    "                if self.thetaprior == \"gamma\":\n",
    "                    if self.kerntype[i] == 'stdp': # pbc\n",
    "                        shape = 3.3678\n",
    "                        rate = 9.0204\n",
    "                    else: # nonpbc\n",
    "                        shape, rate = Distributions.gammaparams(\n",
    "                            self.periods[i]/20, self.periods[i]/2\n",
    "                                                 )\n",
    "                    self.thetapriorpar.append([shape, rate])\n",
    "                else:\n",
    "                    raise TypeError(\"Unknown options set for \\\n",
    "                                    K_priortype: '\" + self.thetaprior + \"'.\")\n",
    "\n",
    "# default initial hyperparameters\n",
    "        if self.thetainit is None:\n",
    "            self.thetainit = [1] # var fixed, change kernel constructer later so that it does not need\n",
    "            for i in range(self.dim): # lengthscales\n",
    "                if self.kerntype[i] == 'stdp': # pbc\n",
    "                    self.thetainit.append(np.pi/10)\n",
    "                else: # nonpbc\n",
    "                    self.thetainit.append(self.periods[i]/20)\n",
    "\n",
    "\n",
    "### BOSS MT\n",
    "self.kernel = GPy.util.multioutput.LCM(dim, num_tasks, kernels_list, W_rank)\n",
    "        # set kernel variance fixed to 1 (gpy sets the same variance for all kernels)\n",
    "self.kernel.kern.variance.constrain_fixed(1)\n",
    "\n",
    "### multioutput\n",
    "def ICM(input_dim, num_outputs, kernel, W_rank=1,W=None,kappa=None,name='ICM'):\n",
    "    \"\"\"\n",
    "    Builds a kernel for an Intrinsic Coregionalization Model\n",
    "\n",
    "    :input_dim: Input dimensionality (does not include dimension of indices)\n",
    "    :num_outputs: Number of outputs\n",
    "    :param kernel: kernel that will be multiplied by the coregionalize kernel (matrix B).\n",
    "    :type kernel: a GPy kernel\n",
    "    :param W_rank: number tuples of the corregionalization parameters 'W'\n",
    "    :type W_rank: integer\n",
    "    \"\"\"\n",
    "    if kernel.input_dim != input_dim:\n",
    "        kernel.input_dim = input_dim\n",
    "        warnings.warn(\"kernel's input dimension overwritten to fit input_dim parameter.\")\n",
    "\n",
    "    K = kernel.prod(GPy.kern.Coregionalize(1, num_outputs, active_dims=[input_dim], rank=W_rank,W=W,kappa=kappa,name='B'),name=name)\n",
    "    return K\n",
    "def LCM(input_dim, num_outputs, kernels_list, W_rank=1,name='ICM'):\n",
    "    \"\"\"\n",
    "    Builds a kernel for an Linear Coregionalization Model\n",
    "\n",
    "    :input_dim: Input dimensionality (does not include dimension of indices)\n",
    "    :num_outputs: Number of outputs\n",
    "    :param kernel: kernel that will be multiplied by the coregionalize kernel (matrix B).\n",
    "    :type kernel: a GPy kernel\n",
    "    :param W_rank: number tuples of the corregionalization parameters 'W'\n",
    "    :type W_rank: integer\n",
    "    \"\"\"\n",
    "    Nk = len(kernels_list)\n",
    "    K = ICM(input_dim,num_outputs,kernels_list[0],W_rank,name='%s%s' %(name,0))\n",
    "    j = 1\n",
    "    for kernel in kernels_list[1:]:\n",
    "        K += ICM(input_dim,num_outputs,kernel,W_rank,name='%s%s' %(name,j))\n",
    "        j += 1\n",
    "    return K\n",
    "### coregionalize\n",
    "\n",
    "def __init__(self, input_dim, output_dim, rank=1, W=None, kappa=None, active_dims=None, name='coregion'):\n",
    "        super(Coregionalize, self).__init__(input_dim, active_dims, name=name)\n",
    "        self.output_dim = output_dim\n",
    "        self.rank = rank\n",
    "        if self.rank>output_dim:\n",
    "            print(\"Warning: Unusual choice of rank, it should normally be less than the output_dim.\")\n",
    "        if W is None:\n",
    "            W = 0.5*np.random.randn(self.output_dim, self.rank)/np.sqrt(self.rank)\n",
    "        else:\n",
    "            assert W.shape==(self.output_dim, self.rank)\n",
    "        self.W = Param('W', W)\n",
    "        if kappa is None:\n",
    "            kappa = 0.5*np.ones(self.output_dim)\n",
    "        else:\n",
    "            assert kappa.shape==(self.output_dim, )\n",
    "        self.kappa = Param('kappa', kappa, Logexp())\n",
    "        self.link_parameters(self.W, self.kappa)\n",
    "\n",
    "[docs]    def parameters_changed(self):\n",
    "        self.B = np.dot(self.W, self.W.T) + np.diag(self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOSS-MT (as should)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
